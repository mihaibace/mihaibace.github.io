@article{strohm25_tiis,
  title = {{HAIFAI}: {Human}-{AI} {Interaction} for {Mental} {Face} {Reconstruction}},
  shorttitle = {{HAIFAI}},
  url = {http://arxiv.org/abs/2412.06323},
  doi = {10.1145/3725891},
  abstract = {We present HAIFAI — a novel two-stage system where humans and AI interact to tackle the challenging task of reconstructing a visual representation of a face that exists only in a person’s mind. In the first stage, users iteratively rank images our reconstruction system presents based on their resemblance to a mental image. These rankings, in turn, allow the system to extract relevant image features, fuse them into a unified feature vector and use a generative model to produce an initial reconstruction of the mental image. The second stage leverages an existing face editing method, allowing users to manually refine and further improve this reconstruction using an easy-to-use slider interface for face shape manipulation. To avoid the need for tedious human data collection for training the reconstruction system, we introduce a computational user model of human ranking behaviour. For this, we collected a small face ranking dataset through an online crowd-sourcing study containing data from 275 participants. We evaluate HAIFAI and an ablated version in a 12-participant user study and demonstrate that our approach outperforms the previous state of the art regarding reconstruction quality, usability, perceived workload and reconstruction speed. We further validate the reconstructions in a subsequent face ranking study with 18 participants and show that HAIFAI achieves a new state-of-the-art identification rate of 60.6%. These findings represent a significant advancement towards developing new interactive intelligent systems capable of reliably and effortlessly reconstructing a user’s mental image.},
  journal = {ACM Transactions on Interactive Intelligent Systems (TiiS)},
  author = {Strohm, Florian and Bâce, Mihai and Bulling, Andreas},
  keywords = {journal},
  year = {2025},
  pdf = {strohm25_tiis.pdf},
  preview = {strohm25_tiis_teaser.png},
}

@article{jiao25_tiis,
author = {Jiao, Chuhan and Wang, Yao and Zhang, Guanhua and B\^{a}ce, Mihai and Hu, Zhiming and Bulling, Andreas},
title = {DiffGaze: A Diffusion Model for Modelling Fine-grained Human Gaze Behaviour on 360° Images},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2160-6455},
url = {https://doi.org/10.1145/3772075},
doi = {10.1145/3772075},
abstract = {Modelling human gaze behaviour on 360 ({}^{circ})  images is important for various human-computer interaction applications. However, existing methods are limited to predicting discrete fixation sequences or aggregated saliency maps, thereby neglecting fine-grained gaze behaviour such as saccadic eye movements that can be captured by commercial eye-trackers. We introduce a more challenging task—fine-grained gaze sequence generation. This task aims to generate eye-tracker-like gaze data for given stimuli. We propose DiffGaze, a diffusion-based method for generating realistic and diverse fine-grained human gaze sequences conditioned on 360 ({}^{circ})  images. We evaluate DiffGaze on two 360 ({}^{circ})  image benchmarks for fine-grained gaze sequence generation as well as two downstream tasks, scanpath prediction and saliency prediction. Our evaluations show that DiffGaze outperforms the fine-grained gaze generation baselines in all tasks on both benchmarks. We also report a 21-participant survey study showing that our method generates gaze sequences that are indistinguishable from real human sequences. Taken together, our evaluations not only demonstrate the effectiveness of DiffGaze but also point towards a new generation of methods that faithfully model the rich spatial and temporal nature of natural human gaze behaviour.},
journal = {ACM Transactions on Interactive Intelligent Systems (TiiS)},
month = oct,
keywords = {journal},
pdf = {jiao25_tiis.pdf},
}

@techreport{strohm24_arxiv_2,
  title = {SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing},
  author = {Strohm, Florian and Bâce, Mihai and Kaltenecker, Markus and Bulling, Andreas},
  keywords = {report},
  year = {2024},
  pages = {1--18},
  url = {https://arxiv.org/abs/2403.13972},
  doi = {2403.13972},
  abstract = {We propose Semantic Facial Feature Control (SeFFeC) - a novel method for fine-grained face shape editing. Our method enables the manipulation of human-understandable, semantic face features, such as nose length or mouth width, which are defined by different groups of facial landmarks. In contrast to existing methods, the use of facial landmarks enables precise measurement of the facial features, which then enables training SeFFeC without any manually annotated labels. SeFFeC consists of a transformer-based encoder network that takes a latent vector of a pre-trained generative model and a facial feature embedding as input, and learns to modify the latent vector to perform the desired face edit operation. To ensure that the desired feature measurement is changed towards the target value without altering uncorrelated features, we introduced a novel semantic face feature loss. Qualitative and quantitative results show that SeFFeC enables precise and fine-grained control of 23 facial features, some of which could not previously be controlled by other methods, without requiring manual annotations. Unlike existing methods, SeFFeC also provides deterministic control over the exact values of the facial features and more localised and disentangled face edits.},

}

@misc{strohm2024upfaceuserpredictablefinegrainedface,
      title={UP-FacE: User-predictable Fine-grained Face Shape Editing}, 
      author={Florian Strohm and Mihai Bâce and Andreas Bulling},
      keywords = {report},
      year={2024},
      eprint={2403.13972},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.13972}, 
}

@article{penzkofer24_ncaa,
  author = {Penzkofer, Anna and Schaefer, Simon and Strohm, Florian and Bâce, Mihai and Leutenegger, Stefan and Bulling, Andreas},
  keywords = {journal},
  title = {Int-HRL: Towards Intention-based Hierarchical Reinforcement Learning},
  journal = {Neural Computing and Applications (NCAA)},
  year = {2024},
  pages = {1--7},
  doi = {10.1007/s00521-024-10596-2},
  volume = {36},
  issue = {}
}

@article{strohm24_etra,
  title = {Learning User Embeddings from Human Gaze for Personalised Saliency Prediction},
  author = {Strohm, Florian and Bâce, Mihai and Bulling, Andreas},
keywords = {journal},
  year = {2024},
  journal = {Proc. ACM on Human-Computer Interaction (PACM HCI)},
  pages = {1--18},
  volume = {8},
  number = {ETRA},
  doi = {10.1145/3655603}
}

@article{wang24_etra,
  title = {VisRecall++: Analysing and Predicting Visualisation Recallability from Gaze Behaviour},
  author = {Wang, Yao and Jiang, Yue and Hu, Zhiming and Ruhdorfer, Constantin and Bâce, Mihai and Bulling, Andreas},
keywords = {journal},
  year = {2024},
  journal = {Proc. ACM on Human-Computer Interaction (PACM HCI)},
  pages = {1--18},
  volume = {8},
  number = {ETRA},
  doi = {10.1145/3655613}
}

@inproceedings{wang24_chi,
  title = {SalChartQA: Question-driven Saliency on Information Visualisations},
  author = {Wang, Yao and Wang, Weitian and Abdelhafez, Abdullah and Elfares, Mayar and Hu, Zhiming and B{\^a}ce, Mihai and Bulling, Andreas},
keywords = {conf},
  year = {2024},
  pages = {1--14},
  booktitle = {Proc. ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)},
  doi = {10.1145/3613904.3642942}
}

@inproceedings{zhang24_chi,
  title = {Mouse2Vec: Learning Reusable Semantic Representations of Mouse Behaviour},
  author = {Zhang, Guanhua and Hu, Zhiming and B{\^a}ce, Mihai and Bulling, Andreas},
keywords = {conf},
  year = {2024},
  pages = {1--17},
  booktitle = {Proc. ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)},
  doi = {10.1145/3613904.3642141}
}

@inproceedings{wang24_etras,
  title = {Saliency3D: a 3D Saliency Dataset Collected on Screen},
  author = {Wang, Yao and Dai, Qi and B{\^a}ce, Mihai and Klein, Karsten and Bulling, Andreas},
keywords = {conf},
  year = {2024},
  pages = {1--9},
  booktitle = {Proc. ACM International Symposium on Eye Tracking Research and Applications (ETRA)},
  doi = {10.1145/3649902.3653350}
}

@inproceedings{strohm23_uist,
  author = {Strohm, Florian and B{\^a}ce, Mihai and Bulling, Andreas},
  title = {Usable and Fast Interactive Mental Face Reconstruction},
  booktitle = {Proc. ACM Symposium on User Interface Software and Technology (UIST)},
  keywords = {conf},
  year = {2023},
  pages = {1--15},
  doi = {https://doi.org/10.1145/3586183.3606795}
}

@inproceedings{jiao23_uist,
  author = {Jiao, Chuhan and Hu, Zhiming and B{\^a}ce, Mihai and Bulling, Andreas},
  title = {SUPREYES: SUPer Resolution for EYES Using Implicit Neural Representation Learning},
  booktitle = {Proc. ACM Symposium on User Interface Software and Technology (UIST)},
  year = {2023},
  pages = {1--13},
  doi = {10.1145/3586183.3606780},
  keywords = {conf}
}

@inproceedings{penzkofer23_inthrl,
  author = {Penzkofer, Anna and Schaefer, Simon and Strohm, Florian and Bâce, Mihai and Leutenegger, Stefan and Bulling, Andreas},
keywords = {conf},
  title = {Int-HRL: Towards Intention-based Hierarchical Reinforcement Learning},
  booktitle = {Proc. Adaptive and Learning Agents Workshop (ALA)},
  year = {2023},
  pages = {1--7}
}

@inproceedings{sood23_gaze,
  author = {Sood, Ekta and Kögel, Fabian and Müller, Philipp and Thomas, Dominike and Bâce, Mihai and Bulling, Andreas},
  keywords = {conf},
  title = {Multimodal Integration of Human-Like Attention in Visual Question Answering},
  booktitle = {Proc. Workshop on Gaze Estimation and Prediction in the Wild (GAZE), CVPRW},
  year = {2023},
  pages = {2647--2657},
  url = {https://openaccess.thecvf.com/content/CVPR2023W/GAZE/papers/Sood_Multimodal_Integration_of_Human-Like_Attention_in_Visual_Question_Answering_CVPRW_2023_paper.pdf}
}

@article{wang23_tvcg,
  title = {Scanpath Prediction on Information Visualisations},
  author = {Wang, Yao and Bâce, Mihai and Bulling, Andreas},
  year = {2023},
  journal = {IEEE Transactions on Visualization and Computer Graphics (TVCG)},
  keywords = {journal},
  volume = {30},
  number = {7},
  pages = {3902--3914},
  doi = {10.1109/TVCG.2023.3242293}
}

@inproceedings{zhang23_interact,
  title = {Exploring Natural Language Processing Methods for Interactive Behaviour Modelling},
  author = {Zhang, Guanhua and Bortoletto, Matteo and Hu, Zhiming and Shi, Lei and B{\^a}ce, Mihai and Bulling, Andreas},
  booktitle = {Proc. IFIP TC13 Conference on Human-Computer Interaction (INTERACT)},
  pages = {1--22},
  keywords = {conf},
  year = {2023},
  publisher = {Springer}
}

@article{bace19_arxiv,
  title={Accurate and Robust Eye Contact Detection During Everyday Mobile Device Interactions},
  keywords = {report},
  author={Mihai B{\^a}ce and Sander Staal and Andreas Bulling},
  journal={ArXiv},
  year={2019},
  pages = {1--12},
  volume={abs/1907.11115}
}

@article{wang22_tvcg,
  title = {VisRecall: Quantifying Information Visualisation Recallability via Question Answering},
  author = {Wang, Yao and Jiao, Chuhan and Bâce, Mihai and Bulling, Andreas},
  keywords = {journal},
  year = {2022},
  pages = {1--12},
  journal = {IEEE Transactions on Visualization and Computer Graphics (TVCG)},
  doi = {10.1109/TVCG.2022.3198163}
}

@article{bace20_pcm,
  title = {How far are we from quantifying visual attention in mobile HCI?},
  author = {B{\^a}ce, Mihai and Staal, Sander and Bulling, Andreas},
  journal = {IEEE Pervasive Computing},
  keywords = {journal},
  year = {2020},
  volume = {19},
  number = {2},
  doi = {10.1109/MPRV.2020.2967736},
  pages = {46-55}
}

@article{bace17_ahr,
  title={Collocated Multi-user Gestural Interactions with Unmodified Wearable Devices},
  keywords = {journal},
  author={Mihai B{\^a}ce and Sander Staal and G{\'a}bor S{\"o}r{\"o}s and Giorgio Corbellini},
  journal={Augmented Human Research},
  year={2017},
  volume={2},
  pages={1-14}
}

@inproceedings{wang22_etvis,
  title = {Impact of Gaze Uncertainty on AOIs in Information Visualisations},
  author = {Wang, Yao and Koch, Maurice and B{\^a}ce, Mihai and Weiskopf, Daniel and Bulling, Andreas},
  year = {2022},
  pages = {1--6},
  keywords = {conf},
  booktitle = {ETRA Workshop on Eye Tracking and Visualization (ETVIS)},
  doi = {10.1145/3517031.3531166}
}

@inproceedings{strohm22_gmml,
  title = {Facial Composite Generation with Iterative Human Feedback},
  author = {Strohm, Florian and Sood, Ekta and Thomas, Dominike and Bâce, Mihai and Bulling, Andreas},
  year = {2022},
  keywords = {conf},
  booktitle = {Proceedings of the NeurIPS Workshop Gaze Meets ML (GMML)}
}

@inproceedings{mueller22_chi,
  title = {Designing for Noticeability: The Impact of Visual Importance on Desktop Notifications},
  author = {Müller, Philipp and Staal, Sander and B{\^a}ce, Mihai and Bulling, Andreas},
  keywords = {conf},
  year = {2022},
  pages = {1--13},
  booktitle = {Proc. ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)},
  doi = {10.1145/3491102.3501954}
}

@inproceedings{bace22_pets,
  title = {PrivacyScout: Assessing Vulnerability to Shoulder Surfing on Mobile Devices},
  author = {B{\^a}ce, Mihai and Saad, Alia and Khamis, Mohamed and Schneegass, Stefan and Bulling, Andreas},
  keywords = {conf},
  year = {2022},
  booktitle = {Proc. on Privacy Enhancing Technologies (PETs)},
  doi = {10.56553/popets-2022-0090},
  pages = {650--669},
  issue = {3}
}

@inproceedings{zhang22_caugaui,
  author = {Zhang, Guanhua and Hindennach, Susanne and Leusmann, Jan and Bühler, Felix and Steuerlein, Benedict and Mayer, Sven and Bâce, Mihai and Bulling, Andreas},
  keywords = {conf},
  title = {Predicting Next Actions and Latent Intents during Text Formatting},
  booktitle = {Proceedings of the CHI Workshop Computational Approaches for
  Understanding, Generating, and Adapting User Interfaces},
  year = {2022},
  pages = {1--6}
}

@inproceedings{abdessaied22_coling,
  author = {Abdessaied, Adnen and Bâce, Mihai and Bulling, Andreas},
  title = {Neuro-Symbolic Visual Dialog},
  keywords = {conf},
  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics (COLING)},
  year = {2022},
  pages = {1--11}
}

@inproceedings{strohm21_iccv,
  title = {Neural Photofit: Gaze-based Mental Image Reconstruction},
  author = {Strohm, Florian and Sood, Ekta and Mayer, Sven and Müller, Philipp and Bâce, Mihai and Bulling, Andreas},
  keywords = {conf},
  year = {2021},
  booktitle = {Proc. IEEE International Conference on Computer Vision (ICCV)},
  doi = {10.1109/ICCV48922.2021.00031},
  pages = {245-254}
}

@inproceedings{bace20_etra,
  title = {Combining Gaze Estimation and Optical Flow for Pursuits Interaction},
  author = {B{\^a}ce, Mihai and Becker, Vincent and Wang, Chenyang and Bulling, Andreas},
  keywords = {conf},
  year = {2020},
  booktitle = {Proc. ACM International Symposium on Eye Tracking Research and Applications (ETRA)},
  doi = {10.1145/3379155.3391315},
  pages = {1-10}
}

@inproceedings{bace20_chi,
  title = {Quantification of Users' Visual Attention During Everyday Mobile Device Interactions},
  keywords = {conf},
  author = {B{\^a}ce, Mihai and Staal, Sander and Bulling, Andreas},
  year = {2020},
  pages = {1--14},
  booktitle = {Proc. ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)},
  doi = {10.1145/3313831.3376449},
  news = {https://ethz.ch/en/news-and-events/eth-news/news/2020/09/our-actual-attention-is-now-measurable.html},
  video = {https://www.youtube.com/watch?v=SzLn3LujIqw}
}

@inproceedings{bace18_etra,
  author = {B{\^a}ce, Mihai and Staal, Sander and S{\"o}r{\"o}s, G{\'a}bor},
  title = {Wearable Eye Tracker Calibration at Your Fingertips},
  year = {2018},
  keywords = {conf},
  booktitle = {ACM Symposium on Eye Tracking Research \& Applications},
  address = {Warsaw, Poland},
  series = {ETRA '18},
  publisher = {ACM},
  doi = {10.1145/3204493.3204592}
}

@inproceedings{bace17_mobilehci,
  author = {B{\^a}ce, Mihai},
  title = {Augmenting Human Interaction Capabilities with Proximity, Natural Gestures, and Eye Gaze},
  keywords = {conf},
  year = {2017},
  booktitle = {Proceedings of the 19th ACM International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI 2017)},
  address = {Vienna, Austria},
  series = {MobileHCI '17},
  publisher = {ACM},
  isbn = {978-1-4503-4835-5},
  doi = {10.1145/3098279.3119924}
}

@inproceedings{bace17_mobilehci_2,
  author = {B{\^a}ce, Mihai and Schlattner, Philippe and Becker, Vincent and S{\"o}r{\"o}s, G{\'a}bor},
  keywords = {conf},
  title = {Facilitating Object Detection and Recognition through Eye Gaze},
  year = {2017},
  booktitle = {Proceedings of the Workshop on Object Recognition for Input and Mobile Interaction at the 19th ACM International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI 2017)},
  keywords = {conf},
  address = {Vienna, Austria},
  series = {MobileHCI '17},
  publisher = {ACM},
  isbn = {978-1-4503-4835-5},
  doi = {10.3929/ethz-b-000221545}
}

@inproceedings{bace17_ah,
  author = {B{\^a}ce, Mihai and S{\"o}r{\"o}s, G{\'a}bor and Staal, Sander and Corbellini, Giorgio},
  title = {HandshakAR: Wearable Augmented Reality System for Effortless Information Sharing},
  keywords = {conf},
  year = {2017},
  booktitle = {Proceedings of the Augmented Human 2017 Conference (AH 2017)},
  address = {Mountain View, CA, USA},
  series = {AH '17},
  publisher = {ACM},
  isbn = {978-1-4503-4835-5},
  doi = {10.1145/3041164.3041203}
}

@inproceedings{bace16_mgia,
  author = {B{\^a}ce, Mihai and Lepp{\"a}nen, Teemu and Gomez, Argenis Ramirez and de Gomez, David Gil},
  title = {ubiGaze: Ubiquitous Augmented Reality Messaging Using Gaze Gestures},
  keywords = {conf},
  year = {2016},
  booktitle = {SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications},
  address = {Macau},
  series = {SA '16},
  pages = {11:1--11:5},
  publisher = {ACM},
  isbn = {978-1-4503-4551-4},
  doi = {10.1145/2999508.2999530}
}

@inproceedings{bace15_wmnc,
  author = {B{\^a}ce, Mihai and Pignolet, Yvonne-Anne},
  title = {Lightweight Indoor Localization System},
  keywords = {conf},
  year = {2015},
  booktitle = {Proceedings of the 8th IFIP Wireless and Mobile Networking Conference (WMNC 2015). Munich, Germany.},
  pages = {160--167},
  doi = {10.1109/WMNC.2015.22}
}

@inproceedings{bace11_iv,
  author = {Popescu, Voichita and B{\^a}ce, Mihai and Nedevschi, Sergiu},
  keywords = {conf},
  title = {Lane identification and ego-vehicle accurate global positioning in intersections},
  year = {2011},
  booktitle = {Intelligent Vehicles Symposium (IV 2011). Baden-Baden, Germany.},
  pages = {870 - 875},
  publisher = {IEEE},
  isbn = {978-1-4577-0890-9},
  doi = {10.1109/IVS.2011.5940523}
}

@inproceedings{bace11_synasc,
  author = {Popescu, Voichita and B{\^a}ce, Mihai and Nedevschi, Sergiu},
  keywords = {conf},
  title = {Probabilistic Approach for Automated Reasoning for Lane Identification in Intelligent Vehicles},
  year = {2011},
  booktitle = {International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC 2011). Timisoara, Romania.},
  pages = {255 - 258},
  publisher = {IEEE},
  isbn = {978-1-4673-0207-4},
  doi = {10.1109/SYNASC.2011.10}
}